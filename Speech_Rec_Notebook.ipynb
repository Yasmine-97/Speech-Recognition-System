{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to the Speech Recognition Challenge!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"##################################################\n# Imports\n##################################################\n\nimport numpy as np\nimport cv2\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\n\n##################################################\n# Params\n##################################################\n\nDATA_BASE_FOLDER = '/kaggle/input/ml-project-speech-recognition-challenge'\nSAMPLE_RATE = 16000\nHOP_LEN = 512","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset\n\nThe dataset is a reduced version of the [`TensorFlow Speech Commands Dataset`](https://www.tensorflow.org/datasets/catalog/speech_commands) and contains audio waveforms of the words:\n- `down`, \n- `go`, \n- `left`, \n- `off`, \n- `on`, \n- `right`, \n- `stop`, \n- `up`.\n\n\nTrain / Validation Split\n- 1600 train samples, 200 for each class\n- 109 validation samples"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"##################################################\n# Load dataset\n##################################################\n\n# Load annotations\ndf_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))\ndf_validation = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))\n\nlabels = sorted(list(set(df_train['word'].values)))\ny_train = df_train['word'].map(lambda w: labels.index(w)).values\ny_validation = df_validation['word'].map(lambda w: labels.index(w)).values\n\n\n# Load audio\naudio_train = np.load(os.path.join(DATA_BASE_FOLDER, 'train_audio.npy'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction\n\nThe speech is a time series signal and a well known strategy for extracting a good representation of the raw audio is to mimic the processing of the auditory system of the humans. A well established feature representation for speech is the so called \"log mel-spectrum\". This feature in fact, takes into account how humans perceive both the frequencies and the amplitude of the sound logarithmically. If you want to dig more into this topic [here](https://medium.com/@jonathan_hui/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9) you can find some details. \n\n![auditory-system](https://www.researchgate.net/profile/Morteza_Khaleghi_Meybodi/publication/322343133/figure/fig1/AS:581011472093184@1515535337239/Figure-31-Schematic-of-the-auditory-system-with-its-primary-components-including.png)\n\n\nFor this project these features are precomputed: for each audio waveform of 1 sec duration, the log mel-spectrum is a bi-dimensional representation (frequency vs time) of shape [128, 32]. Here, we first resize the \"image\" into a [32, 32] matrix and then we flatten the representation into a 32x32 = 1024 vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Features\nx_train_raw = np.load(os.path.join(DATA_BASE_FOLDER, 'train_feat.npy'))\nx_validation_raw = np.load(os.path.join(DATA_BASE_FOLDER, 'validation_feat.npy'))\n\n# Plot audio feature\nidx = 1205\ntime = np.arange(1, SAMPLE_RATE + 1, HOP_LEN) / SAMPLE_RATE\nplt.figure(figsize=(10, 5))\nplt.title(f'Mel-Spectrogram of audio: {df_train[\"word\"][idx]}', fontweight='bold')\nplt.imshow(x_train_raw[idx], aspect='auto', origin='low', cmap='inferno')\nxticks = plt.xticks()[0].astype(np.int32)\nplt.xticks(xticks[1:-1], [f'{1000 * t:.0f}' for t in time[xticks[1:-1]]])\nplt.xlabel('Time [ms]', fontweight='bold')\nplt.ylabel('Log Mel-Spectogram', fontweight='bold')\nplt.grid(lw=0.4, c='w', alpha=0.4)\nplt.show()\n\n# Play audio\nipd.Audio(audio_train[idx], rate=SAMPLE_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For test set\nx_test_raw = np.load(os.path.join(DATA_BASE_FOLDER, 'test_feat.npy'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resize the features\nx_train = []\nfor x_i in x_train_raw:\n    x_train += [cv2.resize(x_i, (32, 32))]\nx_train = np.array(x_train)\nx_validation = []\nfor x_i in x_validation_raw:\n    x_validation += [cv2.resize(x_i, (32, 32))]\nx_validation = np.array(x_validation)\n\n# Plot audio feature\nidx = 1205\nplt.figure(figsize=(5, 3))\nplt.title(f'Mel-Spectrogram of audio: {df_train[\"word\"][idx]}', fontweight='bold')\nplt.imshow(x_train[idx], aspect='auto', origin='low', cmap='inferno')\nplt.grid(lw=0.4, c='w', alpha=0.4)\nplt.show()\n\n# Play audio\nipd.Audio(audio_train[idx], rate=SAMPLE_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For test set: Resize the features\nx_test = []\nfor x_i in x_test_raw:\n    x_test += [cv2.resize(x_i, (32, 32))]\nx_test = np.array(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Convolutional Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom os import listdir\nfrom os.path import isdir, join\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.layers import BatchNormalization\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reshaping the feature matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we reshape the input vector to a suitable shape for CNN\nprint(\"Shape of x_train before reshaping: \",x_train.shape)\nx_train = x_train.reshape(x_train.shape[0], \n                          x_train.shape[1], \n                          x_train.shape[2], \n                          1)\n\nx_validation = x_validation.reshape(x_validation.shape[0], \n                        x_validation.shape[1], \n                        x_validation.shape[2], \n                        1)\n#reshaping for x_test\nx_test = x_test.reshape(x_test.shape[0], \n                          x_test.shape[1], \n                          x_test.shape[2], \n                          1)\nprint(\"After reshaping x_train\", x_train.shape)\nprint(\"After reshaping x_\", x_validation.shape)\nprint(\"After reshaping x_\", x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Input Shape for CNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#define the sample shape to input for CNN\nsample_shape = x_train.shape[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training phase"},{"metadata":{},"cell_type":"markdown","source":"> As specified in the report, in this section, we tried different techniques such as dropout and batch norm to 2 CNN architectures( 2 layers and 3 layers)"},{"metadata":{},"cell_type":"markdown","source":"The following is the optimal 3 Convolutional layers and 1 Dense Layer with no dropout and batch norm which gives an accuracy of 93% of validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we set the seed to obtain the same results\ntf.random.set_seed(1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we define the layers of the CNN\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, \n                        (2, 2), \n                        activation='relu',\n                        input_shape=sample_shape))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(layers.Conv2D(32, (2, 2), activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(layers.Conv2D(64, (2, 2), activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n# Classifier\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(8, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='sparse_categorical_crossentropy', \n              optimizer='adam', \n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, \n                    y_train, \n                    epochs=30,  \n                    batch_size=50, validation_data=(x_validation, y_validation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##################################################\n# Evaluate the model here\n##################################################\n\n# Use this function to evaluate your model\ndef accuracy(y_pred, y_true):\n    '''\n    input y_pred: ndarray of shape (N,)\n    input y_true: ndarray of shape (N,)\n    '''\n    return (1.0 * (y_pred == y_true)).mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report the accuracy in the train and validation sets.\npred_cnn=model.predict_classes(x_validation)\nacc=accuracy(pred_cnn,y_validation)\nprint(\"the accuracy of CNN is : \",acc)\n#This is the result for Without Dropout no normalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\ntf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network in Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Flatten the features\nx_train = x_train.reshape(x_train.shape[0], -1)\nx_validation = x_validation.reshape(x_validation.shape[0], -1)\nx_test= x_test.reshape(x_test.shape[0], -1)\n\nprint(f'Features dimension size: {x_train.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we set the seed to obtain the same results\ntf.random.set_seed(1223)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_ROWS = 32\nNUM_COLS = 32\nNUM_CLASSES = 8\nBATCH_SIZE = 128\nEPOCHS = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following is the optimal ANN architecture with 2 hidden layers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build neural network\nmodel = models.Sequential()\nmodel.add(layers.Dense(690, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n#model.add(layers.Dense(512, activation='relu')) #uncomment if you want to use three layers (#set.seed(123))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(8, activation='softmax'))\n#for 2 layers set seed 1223\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model\nhistory= model.fit(x_train, y_train,\n          epochs=EPOCHS,\n          verbose=1,batch_size=BATCH_SIZE,\n          validation_data=(x_validation, y_validation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_nn=model.predict_classes(x_validation)\naccnn=accuracy(pred_nn,y_validation)\nprint(\"the accuracy of NN  is : \",accnn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Send the submission for the challenge"},{"metadata":{},"cell_type":"markdown","source":"# Here write the model accurate model and send submission "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save your test prediction in y_test_pred\n##################################################\n\ny_test_pred = pred\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nsubmission.pop('word')\nif y_test_pred is not None:\n    submission['word'] = [labels[int(y_i)] for y_i in y_test_pred]\nsubmission.to_csv('trial3.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}